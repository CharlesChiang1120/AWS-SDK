{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "\n",
    "#Client\n",
    "athena_cli = boto3.client('athena')\n",
    "s3_res = boto3.resource('s3')\n",
    "s3_cli = s3_res.meta.client\n",
    "\n",
    "#Env\n",
    "output_bucket = os.environ['ATHENA_OUTPUT_BUCKET']\n",
    "chunk = os.environ['CHUNK']\n",
    "upload_bucket = os.environ['BUCKET_NAME']\n",
    "\n",
    "#Day before yesterday(2)                                             \n",
    "day_before_yesterday = datetime.now() - timedelta(days=2)\n",
    "\n",
    "#Main Function\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    #SQL Query\n",
    "    sql_query_statement = '''\n",
    "        with clordtrail_log AS (\n",
    "            SELECT\n",
    "                json_extract(responseelements,'$.queryExecutionId') AS query_id,\n",
    "                from_iso8601_timestamp(eventtime) AS datetime\n",
    "            FROM    \n",
    "                \n",
    "            WHERE   \n",
    "                eventsource='athena.amazonaws.com'\n",
    "                AND eventname='StartQueryExecution'\n",
    "                AND json_extract(responseelements, '$.queryExecutionId') is NOT null\n",
    "                                )\n",
    "            SELECT *\n",
    "            FROM   clordtrail_log \n",
    "            WHERE  date_diff('day', datetime, current_date) = 1'''\n",
    "    \n",
    "    #Partition\n",
    "    s3_output = 's3://{}/{}/{}/{}/'.format(output_bucket, datetime.now().year, datetime.now().month, datetime.now().day)  \n",
    "    \n",
    "    #Athena Query\n",
    "    batch_query_id = run_query(sql_query_statement, s3_output, max_execution=5)\n",
    "    print(batch_query_id)\n",
    "    \n",
    "    #batch_get_query_execution limited 50 query id\n",
    "    #Split List into chunk \n",
    "    batch_query_chunks = list(divide_chunks(batch_query_id, int(chunk)))\n",
    "    \n",
    "    batch_query(batch_query_chunks)\n",
    "    \n",
    "def run_query(sql_query, s3_output, max_execution=5):\n",
    "    \n",
    "    query_id_list = [ ]\n",
    "    \n",
    "    #Run SQL Query\n",
    "    response = athena_cli.start_query_execution(\n",
    "                                    QueryString=sql_query,\n",
    "                                    ResultConfiguration={\n",
    "                                    'OutputLocation': s3_output\n",
    "                                                        }\n",
    "                                                )\n",
    "\n",
    "    #Query ID\n",
    "    execution_id = response['QueryExecutionId']\n",
    "    \n",
    "    print(\"QueryExecutionId = \"+str(execution_id))\n",
    "    \n",
    "    state = 'QUEUED'\n",
    "\n",
    "    while (max_execution > 0 and state in ['RUNNING', 'QUEUED']):\n",
    "        max_execution = max_execution - 1\n",
    "        print(\"maxexecution=\" + str(max_execution))\n",
    "        response = athena_cli.get_query_execution(QueryExecutionId=execution_id)  \n",
    "\n",
    "        if 'QueryExecution' in response and \\\n",
    "                'Status' in response['QueryExecution'] and \\\n",
    "                'State' in response['QueryExecution']['Status']:\n",
    "\n",
    "                state = response['QueryExecution']['Status']['State']\n",
    "                print(state)\n",
    "                \n",
    "                if state == 'SUCCEEDED':\n",
    "                    results = athena_cli.get_query_results(QueryExecutionId=execution_id,\n",
    "                                                                MaxResults=1000)  \n",
    "                                                                \n",
    "                    for i in range(1, len(results['ResultSet']['Rows'])):\n",
    "                        query_id_list.extend(re.findall(r'\"([^\"]*)\"',results['ResultSet']['Rows'][i]['Data'][0].get('VarCharValue')))\n",
    "                    \n",
    "                     \n",
    "                elif state=='FAILED' or state=='CANCELLED':\n",
    "                    return False\n",
    "                    \n",
    "        time.sleep(30)\n",
    "    \n",
    "    return query_id_list\n",
    "    \n",
    "#Function about split list into chunk\n",
    "def divide_chunks(batch_query_list:list, chunk:int): \n",
    "\n",
    "    for i in range(0, len(batch_query_list), chunk):  \n",
    "        yield batch_query_list[i: i+chunk] \n",
    "  \n",
    "    \n",
    "def batch_query(batch_query_chunks):\n",
    "    \n",
    "    file = open(\"/tmp/csv_file.csv\", \"w\")\n",
    "    temp_csv_file = csv.writer(file, delimiter='|') \n",
    "    temp_csv_file.writerow([\"Date\", \"QueryExecutionId\", \"State\", \"DataScannedInBytes\", \"TotalExecutionTimeInMillis\", \"QueryStatement\", \"Comment\"])\n",
    "        \n",
    "    \n",
    "    for query_id in range(len(batch_query_chunks)):\n",
    "        print(batch_query_chunks[query_id])\n",
    "        response=athena_cli.batch_get_query_execution(\n",
    "                            QueryExecutionIds=batch_query_chunks[query_id]\n",
    "                                                     )\n",
    "        \n",
    "        for i in range(len(response['QueryExecutions'])):\n",
    "            \n",
    "            queryexecution=response['QueryExecutions']\n",
    "            queryexecution_time=datetime.strftime(day_before_yesterday,'%Y-%m-%d')\n",
    "            \n",
    "            if len(re.findall( r'\\/\\*', queryexecution[i]['Query'].replace('\\r', '').replace('\\n','').replace('|','').replace(' ',''))) != 0:\n",
    "                temp_csv_file.writerow(\n",
    "                                    [\n",
    "                            queryexecution_time,\n",
    "                            queryexecution[i]['QueryExecutionId'],\n",
    "                            str(queryexecution[i]['Status']['State']),\n",
    "                            queryexecution[i]['Statistics']['DataScannedInBytes'],\n",
    "                            queryexecution[i]['Statistics']['TotalExecutionTimeInMillis'],\n",
    "                            str(queryexecution[i]['Query'].replace('\\r', '').replace('|','').replace(' ','').replace('\\n','')),\n",
    "                            1\n",
    "                                    ]\n",
    "                                    )\n",
    "            elif len(re.findall( r'\\-\\-',queryexecution[i]['Query'].replace('\\r', '').replace('\\n', '').replace('|', '').replace(' ', ''))) != 0:\n",
    "                temp_csv_file.writerow(\n",
    "                                    [\n",
    "                            queryexecution_time,\n",
    "                            queryexecution[i]['QueryExecutionId'],\n",
    "                            str(queryexecution[i]['Status']['State']),\n",
    "                            queryexecution[i]['Statistics']['DataScannedInBytes'],\n",
    "                            queryexecution[i]['Statistics']['TotalExecutionTimeInMillis'],\n",
    "                            str(queryexecution[i]['Query'].replace('\\r', '').replace(' ', '').replace('\\n', '').replace('|', '')),\n",
    "                            1\n",
    "                                    ]\n",
    "                                    )\n",
    "            else:\n",
    "                temp_csv_file.writerow(\n",
    "                                    [\n",
    "                            queryexecution_time,\n",
    "                            queryexecution[i]['QueryExecutionId'],\n",
    "                            str(queryexecution[i]['Status']['State']),\n",
    "                            queryexecution[i]['Statistics']['DataScannedInBytes'],\n",
    "                            queryexecution[i]['Statistics']['TotalExecutionTimeInMillis'],\n",
    "                            str(queryexecution[i]['Query'].replace('\\r', '').replace(' ', '').replace('\\n', '').replace('|', '')),\n",
    "                            0\n",
    "                                    ]\n",
    "                                    )\n",
    "                \n",
    "                            \n",
    "                                    \n",
    "    file.close()            \n",
    "                                    \n",
    "    s3_cli.upload_file('/tmp/csv_file.csv', upload_bucket,'Querydate-{}-{}-{}-Uploadtos3Time-{}-{}-{}.csv'.format(day_before_yesterday.year,\\\n",
    "                                                                                                              str(day_before_yesterday.month).zfill(2),\\\n",
    "                                                                                                              str(day_before_yesterday.day).zfill(2),\\\n",
    "                                                                                                              datetime.now().year,\\\n",
    "                                                                                                              datetime.now().month,\\\n",
    "                                                                                                              datetime.now().day))\n",
    "                                                                                                                \n",
    "            \n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
